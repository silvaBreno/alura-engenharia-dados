# Projeto: Pipeline de Dados de Vendas

Este projeto demonstra a constru√ß√£o de um pipeline de dados ETL (Extra√ß√£o, Transforma√ß√£o e Carga) em Python. O objetivo √© consolidar e padronizar dados de vendas de duas fontes distintas (Empresa A e Empresa B), que est√£o em formatos diferentes (JSON e CSV), e unific√°-los em um √∫nico arquivo CSV processado.

## üöÄ Objetivo

O principal objetivo √© criar um processo automatizado para:
1.  **Extrair** dados de um arquivo JSON e um arquivo CSV.
2.  **Transformar** os dados para que tenham um esquema de colunas consistente.
3.  **Tratar** dados ausentes.
4.  **Unificar** os dois conjuntos de dados.
5.  **Carregar** o resultado em um novo arquivo CSV, pronto para an√°lise.

## üìÇ Estrutura do Projeto

O reposit√≥rio est√° organizado da seguinte forma:


-   **`data_raw/`**: Cont√©m os arquivos de dados brutos das empresas A (JSON) e B (CSV).
-   **`data_processed/`**: Armazena o arquivo de sa√≠da do pipeline (`dados_combinados.csv`).
-   **`notebooks/`**: Inclui o Jupyter Notebook (`exploracao.ipynb`) usado para a explora√ß√£o inicial e desenvolvimento da l√≥gica do pipeline.
-   **`scripts/`**: Cont√©m os scripts Python para a execu√ß√£o do pipeline.
-   **`guia_config.md`**: Um guia detalhado para configurar o ambiente de desenvolvimento.

## ‚öôÔ∏è O Processo ETL

O pipeline foi desenvolvido seguindo as tr√™s etapas cl√°ssicas do ETL:

### 1. Extra√ß√£o (Extract)

Os dados s√£o lidos de duas fontes com formatos e estruturas distintas:

-   **Empresa A**: Um arquivo `JSON` (`dados_empresaA.json`) contendo uma lista de objetos, onde cada objeto representa um produto.
-   **Empresa B**: Um arquivo `CSV` (`dados_empresaB.csv`) com os dados de vendas, possuindo nomes de colunas diferentes da Empresa A.

As bibliotecas nativas do Python, `json` e `csv` (especificamente `csv.DictReader`), foram utilizadas para carregar os dados em mem√≥ria como uma lista de dicion√°rios, facilitando a manipula√ß√£o.

### 2. Transforma√ß√£o (Transform)

Esta √© a etapa mais complexa, onde os dados s√£o limpos e padronizados:

-   **Padroniza√ß√£o de Colunas**: Os nomes das colunas do arquivo da Empresa B foram mapeados e renomeados para corresponder ao padr√£o da Empresa A. Isso garante que ambos os conjuntos de dados compartilhem o mesmo esquema.

    *Mapeamento de Colunas:*
    ```python
    key_mapping = {
        'Nome do Item': 'Nome do Produto',
        'Classifica√ß√£o do Produto': 'Categoria do Produto',
        'Valor em Reais (R$)': 'Pre√ßo do Produto (R$)',
        'Quantidade em Estoque': 'Quantidade em Estoque',
        'Nome da Loja': 'Filial',
        'Data da Venda': 'Data da Venda'
    }
    ```

-   **Tratamento de Dados Ausentes**: Foi identificado que os dados da Empresa A n√£o possu√≠am a coluna `Data da Venda`. Para manter a consist√™ncia, essa coluna foi adicionada a todos os registros da Empresa A com o valor padr√£o `"Indispon√≠vel"`. A fun√ß√£o `dict.get(chave, valor_padrao)` foi essencial para essa tarefa.

-   **Unifica√ß√£o dos Dados**: Ap√≥s a padroniza√ß√£o, os dois conjuntos de dados foram combinados em uma √∫nica lista.

### 3. Carga (Load)

O conjunto de dados final, unificado e limpo, √© salvo no diret√≥rio `data_processed/` como um novo arquivo CSV chamado `dados_combinados.csv`. Este arquivo est√° pronto para ser consumido por ferramentas de an√°lise ou business intelligence.

## üí° Evolu√ß√£o do C√≥digo

O desenvolvimento do projeto seguiu uma evolu√ß√£o natural:

1.  **Explora√ß√£o em Notebook**: A l√≥gica inicial foi desenvolvida no Jupyter Notebook `exploracao.ipynb`. Isso permitiu uma an√°lise interativa dos dados e a identifica√ß√£o dos desafios de transforma√ß√£o.

2.  **Script Procedural**: A l√≥gica foi inicialmente portada para um script Python (`fusao_mercado.py`) de forma procedural, com fun√ß√µes separadas para cada etapa do ETL.

3.  **Refatora√ß√£o para Programa√ß√£o Orientada a Objetos (POO)**: Para melhorar a organiza√ß√£o, o reuso e a manutenibilidade do c√≥digo, a l√≥gica foi refatorada de um script procedural (`fusao_mercado.py`) para uma abordagem orientada a objetos.
    -   **Abstra√ß√£o e Encapsulamento**: Foi criada a classe `Dados` no arquivo `processamento_dados.py`. Essa classe **abstrai** a complexidade da manipula√ß√£o dos dados (sejam eles JSON ou CSV) e **encapsula** tanto os dados brutos (`self.dados`) quanto as opera√ß√µes que podem ser realizadas sobre eles (ler, renomear colunas, juntar, salvar).
    -   **Coes√£o e Reusabilidade**: M√©todos como `rename_columns` e `salvando_dados` agora pertencem √† classe `Dados`, tornando o c√≥digo mais coeso e f√°cil de entender. Qualquer programador que utilize a classe n√£o precisa saber *como* os dados s√£o lidos ou salvos, apenas que pode chamar `Dados.leitura_dados()` e `dados_fusao.salvando_dados()`.
    -   **Orquestra√ß√£o Simplificada**: O script final, `fusao_mercado_fev.py`, tornou-se um "orquestrador" do pipeline. Ele utiliza as inst√¢ncias da classe `Dados` para executar as etapas de E-T-L de forma limpa, leg√≠vel e com um alto n√≠vel de abstra√ß√£o, focando no "o qu√™" em vez do "como".

## üõ†Ô∏è Como Executar o Projeto

Para executar o pipeline de dados, siga os passos abaixo. Para um guia mais detalhado sobre a configura√ß√£o do ambiente, consulte o arquivo `guia_config.md`.

1.  **Clone o reposit√≥rio:**
    ```bash
    git clone https://github.com/seu-usuario/pipeline_dados.git
    cd pipeline_dados
    ```

2.  **Crie e ative um ambiente virtual:**
    ```bash
    # Para Linux/macOS
    python3 -m venv .venv
    source .venv/bin/activate

    # Para Windows
    python -m venv .venv
    .\.venv\Scripts\activate
    ```

3.  **Instale as depend√™ncias:**
    ```bash
    pip install -r requirements.txt
    ```

4.  **Execute o pipeline:**
    ```bash
    python scripts/fusao_mercado_fev.py
    ```

Ap√≥s a execu√ß√£o, o arquivo `data_processed/dados_combinados.csv` ser√° criado com os dados unificados.

## üìö Principais Aprendizados

-   Manipula√ß√£o de diferentes formatos de arquivo (JSON e CSV) em Python.
-   T√©cnicas de padroniza√ß√£o de esquemas de dados (renomea√ß√£o de colunas).
-   Estrat√©gias para lidar com dados ausentes ou inconsistentes.
-   Estrutura√ß√£o de um pipeline de ETL simples.
-   **Refatora√ß√£o para Programa√ß√£o Orientada a Objetos (POO)**: Aplica√ß√£o de **abstra√ß√£o** e **encapsulamento** para criar uma classe (`Dados`) coesa e reutiliz√°vel, separando as responsabilidades e simplificando o script principal do pipeline. Isso resultou em um c√≥digo mais limpo e de f√°cil manuten√ß√£o em compara√ß√£o com a abordagem procedural inicial.
-   A import√¢ncia do uso de ambientes virtuais para o gerenciamento de depend√™ncias.
